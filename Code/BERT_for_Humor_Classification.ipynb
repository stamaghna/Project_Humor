{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fine_Tuning_BERT_for_Humor_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFOTiqrtNvyy"
      },
      "source": [
        "# Install Transformers Library\r\n",
        "!pip install transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDBnOFcJtGpG"
      },
      "source": [
        "!pip install contractions\n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hkhc10wNrGt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOeEGtfemi97"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Humor_project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4giRzM7NtHJ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, BertTokenizer,TFBertModel\n",
        "#import data_pre\n",
        "#import Pickle as pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJrQFQgN_BE"
      },
      "source": [
        "df = pd.read_csv ('/content/drive/My Drive/Humor_project/train.csv')\n",
        "df_1 = pd.read_csv('/content/drive/My Drive/Humor_project/new_dataset.csv')\n",
        "#df= pd.concat([df, df_1], ignore_index=True, sort=False)\n",
        "test_df = pd.read_csv('/content/drive/My Drive/Humor_project/dev.csv') \n",
        "#df.tail()\n",
        "df_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2QsDbB_UkYF"
      },
      "source": [
        "df_1_text, df_2_text = train_test_split(df_1, \r\n",
        "                                                                    #random_state=2018, \r\n",
        "                                                                    test_size=0.5, \r\n",
        "                                                                   stratify=df_1['is_humor'])\r\n",
        "#df_1_new = pd.DataFrame([df_1_text,df_1_labels],columns = [\"text\",\"is_humor\"])\r\n",
        "#df_1_new['text'] = (df_1_text)\r\n",
        "#df_1_new['is_humor'] = df_1_labels\r\n",
        "#df_1_new['text'] = pd.Series(df_1_text).values\r\n",
        "#df= pd.concat([df, df_1_text], ignore_index=True, sort=False)\r\n",
        "df_1_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASCj-0tsXaz1"
      },
      "source": [
        "#test_df=pd.read_csv ('/content/drive/My Drive/Humor_project/public_dev.csv')\n",
        "#test_df=pd.read_csv ('/content/drive/My Drive/Humor_project/public_test.csv')\n",
        "#test_df['humor_controversy'] = (test_df['humor_controversy'].fillna(0)).astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgWeL2yvKmR1"
      },
      "source": [
        "#df = df.loc[df['offense_rating'] <=1 ]\n",
        "#df = df.loc[df['humor_controversy'] <=1 ]\n",
        "\n",
        "#df = df[pd.to_numeric(df.is_humor, errors='coerce').notnull()]\n",
        "#df = df[pd.to_numeric(df.humor_rating, errors='coerce').notnull()]\n",
        "#test_df = test_df.loc[test_df['is_humor']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CORrsF7XrC4"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzPPOrVQWiW5"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-woQW0lxcsQ"
      },
      "source": [
        "def clean_humor(df):\r\n",
        "  tempArr=[]\r\n",
        "  for line in df:\r\n",
        "    tmpL=line.lower()\r\n",
        "    tempArr.append(tmpL)\r\n",
        "  return tempArr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTWZsDHAx8AV"
      },
      "source": [
        "#df['text'] = clean_humor(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "676DPU1BOPdp"
      },
      "source": [
        "# check class distribution\n",
        "#df['humor_controversy'] = (df['humor_controversy'].fillna(0)).astype(int)\n",
        "df['is_humor'].value_counts(normalize = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9i3x7_StqQM"
      },
      "source": [
        "#train_text = data_pre.data_processing(df[['text']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgIK-Jz5u8NC"
      },
      "source": [
        "def data_clean(df1):\n",
        "  df1['text'] = df1['text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "  df1['text'] = [' '.join(map(str, l)) for l in df1['text']]\n",
        "  df1['text'] = df1['text'].replace('\\d+', '', regex=True)\n",
        "  #df['text'] = df['text'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "  #test_df['text'] = test_df['text'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "  #tokenized_doc = test_df['text'].apply(lambda x: x.split())\n",
        "  #tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "  #detokenized_doc = [] \n",
        "  #for i in range(len(test_df)): \n",
        "  #  t = ' '.join(tokenized_doc[i]) \n",
        "  #  detokenized_doc.append(t) \n",
        "  #test_df['text'] = detokenized_doc\n",
        "  return df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOKIaf5yxkH1"
      },
      "source": [
        "df = data_clean(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUeiAkP4I1_v"
      },
      "source": [
        "#df1= df[['text','is_humor']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlNz9o2kJG9Z"
      },
      "source": [
        "#database_text= pd.read_csv ('/content/drive/My Drive/Humor_project/database_text.csv')\n",
        "#database = data_clean(database_text)\n",
        "#database_humor= pd.read_csv ('/content/drive/My Drive/Humor_project/database_humor.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBGAz_3LJUWg"
      },
      "source": [
        "#df_all= pd.concat([df1, database], ignore_index=True, sort=False)\n",
        "#df_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKfWnApvOoE7"
      },
      "source": [
        "# Split train dataset into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfhSPF5jOWb7"
      },
      "source": [
        "#train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['is_humor'], \n",
        "                                                                    #random_state=2018, \n",
        "                                                                    #test_size=0.3, \n",
        "                                                                    #stratify=df['is_humor'])\n",
        "\n",
        "#we will use temp_text and temp_labels to create validation and test set\n",
        "#val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                               #random_state=2018, \n",
        "                                                               #test_size=0.5, \n",
        "                                                               # stratify=temp_labels)\n",
        "#test_text = test_df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHCdlnXpMNF2"
      },
      "source": [
        "train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['is_humor'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.1, \n",
        "                                                                    stratify=df['is_humor'])\n",
        "\n",
        "#we will use temp_text and temp_labels to create validation and test set\n",
        "#test_text = test_df['text']\n",
        "test_labels = test_df['is_humor']\n",
        "test_text = test_df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAFK0xX7Xs8V"
      },
      "source": [
        "#train_text, val_text, train_labels, val_labels = train_test_split(df['text'], df['is_humor'], \n",
        "#                                                                    random_state=2, \n",
        "#                                                                    test_size=0.1)\n",
        "                                                                    #stratify=df['is_humor'])\n",
        "\n",
        "#we will use temp_text and temp_labels to create validation and test set\n",
        "#test_text = test_df['text']\n",
        "#test_labels = test_df['humor_controversy']\n",
        "#test_text = test_df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_iR7mhwV_nL"
      },
      "source": [
        "train_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCsiOmV7ITJZ"
      },
      "source": [
        "#extra_data_text= database['text']\n",
        "#extra_data_text[0]\n",
        "#train_text[0]\n",
        "#ac_train_text = np.concatenate([train_text,extra_data_text])\n",
        "#ac_train_text.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fMRznTpKLCF"
      },
      "source": [
        "#extra_data_class= database_humor['is_humor']\n",
        "#ac_train_labels = np.concatenate([train_labels,extra_data_class])\n",
        "#ac_train_labels.shape\n",
        "#ac_train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GU_mpcxIncY"
      },
      "source": [
        "#database.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9jrdsYRtSge"
      },
      "source": [
        "#train_text.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7hsdLoCO7uB"
      },
      "source": [
        "# Import BERT Model and BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1kY3gZjO2RE"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wIYaWI_Prg8"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwbpeN_PMiu"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in df['text']]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXcswEIRPvGe"
      },
      "source": [
        "max_seq_len = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk5S7DWaP2t6"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "#tokenizer = tokenization.FullTokenizer(\n",
        "#    train_test=train_test, do_lower_case=True)\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "#tokenizer = tokenization.FullTokenizer(\n",
        "#   val_test=val_test, do_lower_case=True)\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "#tokenizer = tokenization.FullTokenizer(\n",
        "#   test_test=test_test, do_lower_case=True)\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPF5zFaxp_WR"
      },
      "source": [
        "tokens_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsm8bkRZQTw9"
      },
      "source": [
        "# Convert Integer Sequences to Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR-lXwmzQPd6"
      },
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1cOBlcRLuk"
      },
      "source": [
        "# Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUy9JKFYQYLp"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2HZc5ZYRV28"
      },
      "source": [
        "# Freeze BERT Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZ0MC00RQA_"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ahGBUWRi3X"
      },
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3iEtGyYRd0A"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      self.dropout = nn.Dropout(0.1)      \n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      #self.fc2 = nn.Linear(512,256)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "      #self.fc4 = nn.Linear(64,2)\n",
        "\n",
        "\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "\n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      \n",
        "      #x=self.relu(x)\n",
        "      #x=self.dropout(x)\n",
        "      #x = self.fc3(x)\n",
        "      #x=self.relu(x)\n",
        "      #x=self.dropout(x)\n",
        "      #x=self.fc4\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAJJVuJRliv"
      },
      "source": [
        "model = BERT_Arch(bert)\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taXS0IilRn9J"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9CDpoMQR_rK"
      },
      "source": [
        "# Find Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izY5xH5eR7Ur"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1WvfY2vSGKi"
      },
      "source": [
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My4CA0qaShLq"
      },
      "source": [
        "# Fine-Tune BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rskLk8R_SahS"
      },
      "source": [
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGXovFDlSxB5"
      },
      "source": [
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KZEgxRRTLXG"
      },
      "source": [
        "# Start Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1USGTntS3TS"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrhUc9kTI5a"
      },
      "source": [
        "# Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OacxUyizS8d1"
      },
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4SVftkkTZXA"
      },
      "source": [
        "# Get Predictions for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZl0SZmFTRQA"
      },
      "source": [
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms1ObHZxTYSI"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print (preds)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzLS7rHTp4T"
      },
      "source": [
        "# confusion matrix\n",
        "#pd.crosstab(test_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpX1uTwjUPY6"
      },
      "source": [
        "preds\n",
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm6NfvminSsC"
      },
      "source": [
        "##count = 0\n",
        "#for i in range(len(test_y)):\n",
        "#  if (test_y[i] != preds[i]):\n",
        "#    count+=1\n",
        "#print (count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8DmfaHojTtk"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "f1_score(test_y,preds),precision_score(test_y,preds),confusion_matrix(test_y,preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSJA7QmWmkzo"
      },
      "source": [
        "test = pd.DataFrame()\n",
        "test['index'] = test_df['id']\n",
        "test['is_humor'] = preds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jto32Pm3ryT-"
      },
      "source": [
        "#test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bynyRuzXr8hP"
      },
      "source": [
        "from google.colab import files\n",
        "test.to_csv('public_dev_bert.csv')\n",
        "files.download('public_dev_bert.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN6gsHceIRjC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}