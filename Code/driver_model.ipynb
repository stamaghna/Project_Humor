{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_pre\n",
    "import input_data\n",
    "import model_run\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "df = input_data.get_data ('train_data/train.csv')\n",
    "test_df = pd.read_csv('dev.csv')\n",
    "df['is_humor'].value_counts(normalize = True)\n",
    "test_df['is_humor'].value_counts(normalize = True)\n",
    "df['humor_controversy'] = (df['humor_controversy'].fillna(0)).astype(int)\n",
    "test_df['humor_controversy'] = (test_df['humor_controversy'].fillna(0)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[['text']]\n",
    "y_train = df[['is_humor','humor_rating','humor_controversy','offense_rating']]\n",
    "X_test = test_df[['text']]\n",
    "y_test = test_df[['is_humor','humor_rating','humor_controversy','offense_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break data\n",
    "y_train = y_train.fillna(0)\n",
    "y_test = y_test.fillna(0)\n",
    "train_humor,train_humor_rating,train_humor_contro,train_offense_rating = input_data.break_df(y_train)\n",
    "test_humor,test_humor_rating,test_humor_contro,test_offense_rating = input_data.break_df(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "final_text_ = data_pre.data_processing ( X_train[['text']] )\n",
    "final_x_test_= data_pre.data_processing ( X_test[['text']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 8000,ngram_range=(1,3))\n",
    "final_text = vectorizer.fit_transform(final_text_)\n",
    "final_x_test = vectorizer.fit_transform(final_x_test_)#vocab = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x8000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 79784 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.023041   1.36756974 1.43924177 ... 1.03677139 2.94494703 0.81429832]] aaron\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dist = np.sum(final_text, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print (count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR,SVC\n",
    "#df_run_2 = model_run.run_model(final_text,humor_contro,final_x_test,y_test[['humor_controversy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['text'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "tokenized_doc = test_df['text'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "detokenized_doc = [] \n",
    "for i in range(len(test_df)): \n",
    "    t = ' '.join(tokenized_doc[i]) \n",
    "    detokenized_doc.append(t) \n",
    "test_df['text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparision with BERT\n",
    "\n",
    "bert_df = pd.read_csv('public_dev_bert(2).csv')\n",
    "f1_score( test_df['is_humor'],bert_df[['is_humor']])#,confusion_matrix(y_test['is_humor'],bert_df[['is_humor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df\n",
    "test_df['berthumor'] = bert_df['is_humor']\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score( test_humor,bert_df['is_humor']),precision_score(test_humor,bert_df['is_humor']),confusion_matrix(test_humor,bert_df['is_humor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame()\n",
    "#test['Actual'] = test_df['is_humor']\n",
    "#test['Bert'] = bert_df['is_humor']\n",
    "#test['NB'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.DataFrame()\n",
    "#test['Actual'] = new_X_test['humor_controversy']\n",
    "#test['Bert'] = bert_df['humor_controversy']\n",
    "#test['NB'] = preds\n",
    "#new_X_test.isnull().values.any(),test['Bert'].isnull().values.any(),new_X_test.shape,test.shape,bert_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['try1'] = np.where((test['Bert'] > 0) & (test['NB'] < 1 )\n",
    "                     ,0,test['Bert'])\n",
    "test.head()\n",
    "test.isnull().values.any()\n",
    "\n",
    "#provides a slight improvement\n",
    "f1_score( test_humor,test['try1']),precision_score(test_humor,test['is_humor'])confusion_matrix(test_humor,test['try1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score( new_X_test[['humor_controversy']],test['try1']),confusion_matrix(new_X_test[['humor_controversy']],test['try1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test.shape)\n",
    "new_X_test['humor_controversy'].to_csv('testtry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "f1_score( test_humor,y_pred),precision_score( test_humor,y_pred),confusion_matrix( test_humor,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,4)).fit(df['text'])\n",
    "X_train_vectorized = vect.transform(df['text'])\n",
    "model = SVR(kernel = 'linear',tol = 0.001,C=1)\n",
    "#Changing Scale from 0 to 5 -> 0 to 500\n",
    "df['offense_rating'] = 100 * df['offense_rating']\n",
    "model.fit(X_train_vectorized,df['offense_rating'])\n",
    "y_predicted= model.predict(vect.transform(test_df['text']))\n",
    "y_predicted[y_predicted<0]=0\n",
    "y_predicted[y_predicted>500]=499\n",
    "#df[['humor_rating_predicted']]=y_predicted_svr\n",
    "y_preds = np.around(y_predicted/100,decimals=2) \n",
    "test_df['svroff'] = y_preds\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_df['offense_rating'],test_df['svroff'], squared=False),test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "reg = GradientBoostingRegressor()\n",
    "#y_train['humor_rating'] = 100 * y_train['humor_rating'] \n",
    "reg.fit(X_train_vectorized,df['offense_rating'])\n",
    "gbrpred = reg.predict(vect.transform(test_df['text']))\n",
    "#gbrpred = np.around(gbrpred,decimals=2)\n",
    "gbrpred[gbrpred<10]=0\n",
    "gbrpred[gbrpred>500]=499\n",
    "test_df['gbrpreds'] = gbrpred\n",
    "test_df['gbrpreds'] = np.around(test_df['gbrpreds']/100,decimals=2)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_df['offense_rating'],test_df['gbrpreds'], squared=False),test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering Data\n",
    "\n",
    "X_train = df.loc[lambda x: x['is_humor'] == 1]\n",
    "X_test = test_df.loc[lambda x: x['berthumor'] == 1]\n",
    "y_train = df.loc[lambda x: x['is_humor'] == 1]\n",
    "y_test = test_df.loc[lambda x: x['berthumor'] == 1]\n",
    "y_rest = test_df.loc[lambda x: x['berthumor'] == 0]\n",
    "vect = CountVectorizer(ngram_range=(1,4)).fit(X_train['text'])\n",
    "X_train_vectorized = vect.transform(X_train['text'])\n",
    "\n",
    "model = SVR(kernel = 'linear',tol = 0.001,C=5)\n",
    "#train_humor_rating['humor_rating'] = (train_humor_rating['humor_rating'].fillna(0)).astype(float)\n",
    "#test_humor_rating['humor_rating'] = (test_humor_rating['humor_rating'].fillna(0)).astype(float)\n",
    "y_train['humor_rating'] = 100 * y_train['humor_rating']\n",
    "model.fit(X_train_vectorized,y_train['humor_rating'])\n",
    "y_predicted= model.predict(vect.transform(X_test['text']))\n",
    "y_predicted[y_predicted<0]=0\n",
    "y_predicted[y_predicted>500]=499\n",
    "#df[['humor_rating_predicted']]=y_predicted_svr\n",
    "y_preds = np.around(y_predicted/100,decimals=2) \n",
    "y_test['svrpreds'] = y_preds\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor()\n",
    "#y_train['humor_rating'] = 100 * y_train['humor_rating'] \n",
    "reg.fit(X_train_vectorized,y_train['humor_rating'])\n",
    "gbrpred = reg.predict(vect.transform(X_test['text']))\n",
    "#gbrpred = np.around(gbrpred,decimals=2) \n",
    "y_test['gbrpreds'] = gbrpred\n",
    "y_test['gbrpreds'] = np.around(y_test['gbrpreds']/100,decimals=2)\n",
    "\n",
    "y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_predicted\n",
    "#y_preds = np.around(y_predicted,decimals=2) \n",
    "#y_preds\n",
    "y = pd.concat([y_test, y_rest], ignore_index = False)\n",
    "y=y.sort_values(by=['id'])\n",
    "#print (y)\n",
    "#y.to_csv('check.csv')\n",
    "y = y.fillna(0)\n",
    "#y\n",
    "y_tt = y.loc[lambda x: x['berthumor'] == 1]\n",
    "#y_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "#f1_score( test_humor_contro,y_predicted),precision_score( test_humor_contro,y_predicted)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_tt['humor_rating'],y_tt['svrpreds'], squared=False),mean_squared_error(y_tt['humor_rating'],y_tt['gbrpreds'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_1= RandomForestRegressor(max_depth=10)\n",
    "model_1.fit(final_text,train_offense_rating[['offense_rating']])\n",
    "y_predicted_RFR= model_1.predict(final_x_test)\n",
    "y_preds_RFR = np.around(y_predicted_RFR,decimals=2) \n",
    "mean_squared_error(test_offense_rating,y_preds_RFR, squared=False)\n",
    "y_preds_RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df['predicted'] = y_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#for x in [40,50,60,70,80,90,100,110,120,130,140]:\n",
    "clf = GradientBoostingClassifier(n_estimators=40, learning_rate=2,max_depth=1, random_state=0).fit(train_humor_rating[['humor_rating']],train_humor_contro[['humor_controversy']])\n",
    "#clf.score(X_test, y_test)\n",
    "#y_predicted = clf.predict(y_predicted_svr)\n",
    "#print (f1_score( test_humor_contro['humor_controversy'],y_predicted),precision_score( test_humor_contro['humor_controversy'],y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train, = (df[['id','text']],df[['is_humor','humor_rating','humor_controversy','offense_rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[['humor_controversy']]=y_predicted.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[['humor_rating']]=round(test_df[['humor_rating']],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "XGBModel = XGBRegressor(booster = 'dart')\n",
    "XGBModel.fit(final_text,train_offense_rating[['offense_rating']] , verbose=False)\n",
    "\n",
    "# Get the mean absolute error on the validation data :\n",
    "XGBpredictions = XGBModel.predict(final_x_test)\n",
    "mean_squared_error(test_offense_rating,XGBpredictions, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proverb_df = pd.read_pickle('datasets/proverbs.pickle')\n",
    "proverb['text']= pd.DataFrame(proverb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[['text']] = pickled_df[['text']]\n",
    "source['is_humor'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle('datasets/humorous_oneliners.pickle')\n",
    "pickled_df[['text']] = pd.DataFrame(unpickled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[['text']] = pickled_df[['text']]\n",
    "source['is_humor'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for other dependencies in dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "#vect = CountVectorizer(ngram_range=(1,4)).fit(X_train['text'])\n",
    "#final_text\n",
    "vect = CountVectorizer(ngram_range=(2,4)).fit(X_train['text'])\n",
    "X_train_vectorized = vect.transform(X_train['text'])\n",
    "#X_train_vectorized = vect.transform(X_train['text'])\n",
    "clfrNB2 = MultinomialNB()\n",
    "#clfrNB = GaussianNB()\n",
    "\n",
    "clfrNB2.fit([X_train_vectorized,train_humor[['is_humor']]],train_humor_contro[['humor_controversy']])\n",
    "#clfrNB.fit(X_train_vectorized,train_humor[['is_humor']])\n",
    "preds2 = clfrNB2.predict(vect.transform(X_test['text']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score( test_humor_contro,preds2),precision_score( test_humor_contro,preds2),confusion_matrix( test_humor_contro,preds2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
